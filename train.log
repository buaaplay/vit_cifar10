Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
Received arguments: ['src/train.py', '--data-dir', '/home/lpya/projects/datasets', '--batch-size', '128', '--epochs', '100', '--lr', '3e-4', '--weight-decay', '5e-2', '--accumulation-steps', '4', '--checkpoint-dir', './checkpoints']
2025-01-06 00:11:54,909 | INFO | 开始执行训练脚本...
2025-01-06 00:11:54,910 | INFO | 日志将保存到 ./logs/batch128_lr0.0003_wd0.05_modelvit_base_patch16_224_20250106_001154
2025-01-06 00:11:54,910 | INFO | 训练参数: {'data_dir': '/home/lpya/projects/datasets', 'batch_size': 128, 'epochs': 100, 'lr': 0.0003, 'weight_decay': 0.05, 'checkpoint_dir': './checkpoints', 'accumulation_steps': 4, 'patience': 10, 'model_name': 'vit_base_patch16_224'}
2025-01-06 00:11:54,910 | INFO | 开始时间: 2025-01-06 00:11:54
2025-01-06 00:11:54,910 | INFO | 加载数据...
2025-01-06 00:11:54,910 | INFO | 加载数据...
2025-01-06 00:11:54,918 | INFO | 加载数据...
2025-01-06 00:11:54,923 | INFO | 加载数据...
2025-01-06 00:11:54,924 | INFO | 加载数据...
2025-01-06 00:11:54,932 | INFO | 加载数据...
2025-01-06 00:11:54,934 | INFO | 加载数据...
2025-01-06 00:11:54,935 | INFO | 加载数据...
Files already downloaded and verifiedFiles already downloaded and verified

Files already downloaded and verifiedFiles already downloaded and verified

Files already downloaded and verifiedFiles already downloaded and verified

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified


Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
2025-01-06 00:11:56,685 | INFO | 构建模型...
2025-01-06 00:11:56,685 | INFO | 构建模型...
2025-01-06 00:11:56,715 | INFO | 构建模型...
2025-01-06 00:11:56,715 | INFO | 构建模型...
2025-01-06 00:11:56,734 | INFO | 构建模型...
2025-01-06 00:11:56,750 | INFO | 构建模型...
2025-01-06 00:11:56,751 | INFO | 构建模型...
2025-01-06 00:11:56,757 | INFO | 构建模型...
2025-01-06 00:11:58,363 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,488 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,617 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,628 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,683 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,754 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,786 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:58,869 | INFO | 转换为 SyncBatchNorm 并使用 DistributedDataParallel...
2025-01-06 00:11:59,672 | INFO | 设置损失函数和优化器...
2025-01-06 00:11:59,672 | INFO | 设置损失函数和优化器...
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-01-06 00:11:59,673 | INFO | 开始第 1 轮训练。
2025-01-06 00:11:59,673 | INFO | 设置损失函数和优化器...
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-01-06 00:11:59,673 | INFO | 设置损失函数和优化器...
2025-01-06 00:11:59,673 | INFO | 设置损失函数和优化器...
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-01-06 00:11:59,677 | INFO | 设置损失函数和优化器...
2025-01-06 00:11:59,677 | INFO | 设置损失函数和优化器...
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-01-06 00:11:59,683 | INFO | 设置损失函数和优化器...
/home/lpya/projects/vit_cifar10/src/train.py:247: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
2025-01-06 00:12:29,481 | INFO | Epoch [1] Train Loss: 2.9798 | Train Acc: 13.46% | Train Time: 29.81s
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
/home/lpya/projects/vit_cifar10/src/train.py:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
2025-01-06 00:12:32,035 | INFO | Epoch [1] Val Loss: 2.2136 | Val Acc: 16.88% | Val Time: 2.55s
2025-01-06 00:12:32,036 | INFO | Epoch [1] Train Loss: 2.9798 | Train Acc: 13.46% | Train Time: 29.81s | Val Loss: 2.2136 | Val Acc: 16.88% | Val Time: 2.55s | Total Elapsed Time: 32.36s
2025-01-06 00:12:40,046 | INFO | 开始第 2 轮训练。
W0106 00:12:59.730000 22777 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received 2 death signal, shutting down workers
W0106 00:12:59.732000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22793 closing signal SIGINT
W0106 00:12:59.733000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22794 closing signal SIGINT
W0106 00:12:59.735000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22795 closing signal SIGINT
W0106 00:12:59.737000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22796 closing signal SIGINT
W0106 00:12:59.739000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22797 closing signal SIGINT
W0106 00:12:59.759000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22798 closing signal SIGINT
W0106 00:12:59.769000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22799 closing signal SIGINT
W0106 00:12:59.771000 22777 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 22800 closing signal SIGINT
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank5]:     main()
[rank5]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank5]:     train_loss, train_acc, train_time = train_epoch(
[rank5]:                                         ^^^^^^^^^^^^
[rank5]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank5]:     scaler.step(optimizer)
[rank5]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank5]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank5]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank5]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank5]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank5]:                ^^^^^^^^
[rank5]: KeyboardInterrupt
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank1]:     main()
[rank1]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank1]:     train_loss, train_acc, train_time = train_epoch(
[rank1]:                                         ^^^^^^^^^^^^
[rank1]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank1]:     scaler.step(optimizer)
[rank1]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank1]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank1]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank1]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank1]:                ^^^^^^^^
[rank1]: KeyboardInterrupt
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank2]:     main()
[rank2]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank2]:     train_loss, train_acc, train_time = train_epoch(
[rank2]:                                         ^^^^^^^^^^^^
[rank2]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank2]:     scaler.step(optimizer)
[rank2]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank2]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank2]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank2]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank2]:                ^^^^^^^^
[rank2]: KeyboardInterrupt
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank6]:     main()
[rank6]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank6]:     train_loss, train_acc, train_time = train_epoch(
[rank6]:                                         ^^^^^^^^^^^^
[rank6]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank6]:     scaler.step(optimizer)
[rank6]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank6]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank6]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank6]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank6]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank6]:                ^^^^^^^^
[rank6]: KeyboardInterrupt
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank3]:     main()
[rank3]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank3]:     train_loss, train_acc, train_time = train_epoch(
[rank3]:                                         ^^^^^^^^^^^^
[rank3]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank3]:     scaler.step(optimizer)
[rank3]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank3]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank3]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank3]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank3]:                ^^^^^^^^
[rank3]: KeyboardInterrupt
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank7]:     main()
[rank7]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank7]:     train_loss, train_acc, train_time = train_epoch(
[rank7]:                                         ^^^^^^^^^^^^
[rank7]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank7]:     scaler.step(optimizer)
[rank7]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank7]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank7]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank7]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank7]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank7]:                ^^^^^^^^
[rank7]: KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank0]:     main()
[rank0]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank0]:     train_loss, train_acc, train_time = train_epoch(
[rank0]:                                         ^^^^^^^^^^^^
[rank0]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank0]:     scaler.step(optimizer)
[rank0]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank0]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank0]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank0]:                ^^^^^^^^
[rank0]: KeyboardInterrupt
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 307, in <module>
[rank4]:     main()
[rank4]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 259, in main
[rank4]:     train_loss, train_acc, train_time = train_epoch(
[rank4]:                                         ^^^^^^^^^^^^
[rank4]:   File "/home/lpya/projects/vit_cifar10/src/train.py", line 123, in train_epoch
[rank4]:     scaler.step(optimizer)
[rank4]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank4]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank4]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in _maybe_opt_step
[rank4]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 351, in <genexpr>
[rank4]:     if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
[rank4]:                ^^^^^^^^
[rank4]: KeyboardInterrupt
[rank0]:[W106 00:13:00.997181701 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/lpya/miniconda3/envs/vit/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/lpya/miniconda3/envs/vit/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 22777 got signal: 2
